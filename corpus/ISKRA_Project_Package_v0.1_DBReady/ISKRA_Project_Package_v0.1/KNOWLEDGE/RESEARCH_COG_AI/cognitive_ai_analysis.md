# Когнитивные подходы в ИИ: архитектуры IBM Watson, Google LaMDA/Sparrow, Meta Code Llama и проекты AGI в свете моделирования сознания

## Введение: постановка задачи, охват и методология

Когнитивные подходы в искусственном интеллекте стремятся воспроизвести или частично смоделировать структуры и процессы, характерные для человеческого познания: память, внимание, рассуждение, планирование, метакогницию (мышление о мышлении) и самосознание. Настоящий отчет анализирует четыре ключевых направления, оказавших определяющее влияние на поле: IBM Watson как ранний образец индустриальной когнитивной архитектуры, семейство Google LaMDA/Sparrow как диалоговые модели с акцентом на безопасность и правила, Meta Code Llama как специализированная модель генерации кода, а также современные проекты AGI и подходы к оценке когнитивных и «сознательных» способностей моделей. Особое внимание уделено метакогниции, теориям сознания и операционализируемым критериям оценки.

В качестве методологической рамки мы опираемся на два слоя анализа. Во-первых, техническая сторона: архитектурные решения, обучение (включая обучение с подкреплением на человеческих отзывах, RLHF), механизмы безопасности и оценки. Во-вторых, философско-когнитивная сторона: соотнесение архитектур и практик обучения с научными теориями сознания (функциональные, нейрокогнитивные и феноменологические перспективы), а также с метакогнитивными функциями мониторинга и контроля. По ключевым вопросам сознания в ИИ мы опираемся на междисциплинарные обзоры и позиции ведущих исследователей[^3][^4].

В дальнейших разделах мы рассматриваем эволюцию когнитивных архитектур (от IBM Watson к LLM), анализируем LaMDA/Sparrow и Code Llama, сопоставляем подходы к безопасности и согласованию, обсуждаем операционализацию метакогниции и сознания, даем обзор AGI-проектов, предлагаем сравнительную матрицу архитектур и завершаем практическими рекомендациями по проектированию когнитивных ИИ-систем.

Ограничения и пробелы данных. Важные архитектурные детали IBM Watson и Sparrow публично неполны; для Meta Code Llama доступна официальная информация о линейке и возможностях, однако без глубокой реконструкции исходных публикаций на момент подготовки отчета; для LaMDA в нашем корпусе нет верифицируемых первичных технических публикаций, что ограничивает детализацию методологии; часть ссылок на DeepMind AGI Levels отсутствует в предоставленном контексте и требует отдельной проверки.

## Философские и когнитивные основы моделирования сознания в ИИ

Сознание традиционно рассматривается в нескольких взаимодополняющих перспективах. Функциональные подходы понимают сознание как набор вычислительных и регуляторных функций, обеспечивающих адаптивное поведение; феноменологические — как совокупность субъективных переживаний; нейрокогнитивные — как производное от конкретных систем мозга и их взаимодействия[^3][^4]. Для ИИ-архитектур наибольшую операциональную ценность имеют функциональные и нейрокогнитивные взгляды: они позволяют связать наблюдаемые процессы (например, мониторинг уверенности, саморегуляцию, планирование) с кандидатами в «механизмы сознания», доступные инженерной реализации.

Метакогниция — «мышление о мышлении» — выступает центральным мостом между человеческой когнитивной архитектурой и современными LLM. Она включает мониторинг (оценку собственных состояний, уверенности, прогресса) и контроль (управление стратегией, ресурсами, остановкой). В психологии метакогниция изучается через поведенческие тесты уверенности, делегирования и «второго шанса»; аналогичные парадигмы переносятся на оценку LLM[^5][^9].

Критически важно различать феноменальное сознание (наличие субъективных переживаний) и доступ к внутренним сигналам (интроспекция). Даже если модель демонстрирует поведенческие признаки интроспекции (использует сигнал уверенности для делегирования или смены стратегии), это не является доказательством наличия феноменального сознания. Именно поэтому мы будем аккуратно разделять «признаки метакогниции» и гипотезы о «сознательности», опираясь на проверяемые методики и ограничения интерпретации[^3][^4].

Чтобы проиллюстрировать метакогнитивные аспекты, обратимся к двум поведенческим парадигмам, получившим развитие в последних работах.

![Схематическое представление Delegate Game](.pdf_temp/viewrange_chunk_5_21_25_1765963818/images/59m6mg.jpg)

На схеме представлен Delegate Game: модель на каждом вопросе выбирает — ответить самой или «делегировать» ответ «напарнику» с известной точностью. Чтобы максимизировать командный результат, модели необходимо использовать внутренний сигнал уверенности: делегировать трудные вопросы, где ее вероятность ошибки высока, и отвечать на легкие. Эта парадигма минимизирует влияние «поверхностных» подсказок сложности и вынуждает модель опираться на интроспективные сигналы. Как показано в исследованиях, даже современные модели демонстрируют ограниченную, но статистически значимую способность к такому поведению; при этом они часто подвержены влиянию внешних «подсказок» и посттренировочных поведенческих установок[^5].

![Second Chance Game: изменение ответа на основе самооценки](.pdf_temp/viewrange_chunk_4_16_20_1765963830/images/dupbm4.jpg)

Second Chance Game тестирует способность модели предсказать собственный ответ и изменить его в ответ на инструкцию («ваш предыдущий ответ был неверен; выберите другой»). Успех требует наличия у модели «самомодели» — приблизительного знания, какой ответ она дала бы, и готовности корректировать поведение на основании этого знания. Результаты указывают на ограниченную, но воспроизводимую способность ряда моделей к такому самомоделированию; при этом улучшения неоднородны и зависят от посттренировочных настроек и формата задач[^5].

### Теории сознания: функциональные, нейрокогнитивные и феноменологические подходы

Современная наука о сознании предлагает несколько комплементарных оптик. Функциональные трактовки сосредоточены на том, какие вычислительные и регуляторные роли выполняет сознание: интеграция информации, гибкое управление ресурсами, планирование. Нейрокогнитивные модели связывают эти роли с конкретными системами (например, префронтальная кора и метакогнитивный контроль, гиппокамп — с памятью и симуляцией будущих действий). Феноменологические подходы настаивают на нередуцируемости субъективного опыта[^3][^4]. Для инженерной практики разумно держаться функционально-нейрокогнитивного уровня: проектировать и тестировать механизмы, реализующие мониторинг и контроль, а claims о «сознании» ограничивать операциональными рамками, избегая категорических феноменологических утверждений без соответствующих доказательств.

### Метакогниция: мониторинг и контроль

Классические тесты метакогниции у животных и людей измеряют способность оценивать собственную уверенность и использовать ее для выбора стратегии (ответить, пропустить, делегировать), а также предвидеть собственные действия («подготовка к будущему»)[^9]. Парадигмы Delegate Game и Second Chance Game переносят эту логику на LLM, фокусируясь на поведении, а не на вербальных самоотчетах, которые могут быть искажены обучением под «правдоподобные ответы»[^5]. Результаты показывают:

- ограниченную, но воспроизводимую способность ряда моделей учитывать внутренние сигналы уверенности при принятии решений;
- сильную чувствительность к посттренировочным поведенческим установкам («личности») и к поверхностным признакам сложности вопроса;
- различия между интроспекцией (оценка уверенности) и самомоделированием (предсказание собственного ответа), что указывает на неоднородность метакогнитивных механизмов у разных моделей[^5].

Эти выводы согласуются с нейрокогнитивными данными о распределении метакогнитивного контроля в мозге и подчеркивают необходимость архитектурного разделения «объектного» уровня (решение задачи) и «мета-уровня» (планирование и регулирование), о чем свидетельствуют новые инженерные фреймворки (см. Meta-R1)[^19].

## Обзор систем: IBM Watson, Google LaMDA/Sparrow, Meta Code Llama и AGI-проекты

### IBM Watson: архитектура когнитивных вычислений

IBM Watson стал одним из первых индустриально масштабируемых образцов когнитивной архитектуры, сочетающих извлечение знаний, рассуждения и интеграцию сервисов для ответов на вопросы в ограниченных доменах. Он продемонстрировал важный инженерный принцип: когнитивная система — это не монолитная модель, а ансамбль компонентов (поиск, ранжирование, знания, логический вывод), взаимодействие которых определяет итоговую «интеллектуальность». Несмотря на успех, современным LLM не хватает ключевых аспектов встроенной метакогнитивной регуляции: явного планирования, онлайнового мониторинга ошибок и гибкой остановки рассуждений. В результате возникают проблемы надежности и эффективности, особенно в длинных цепочках рассуждений, где ошибки накапливаются, а модели не умеют «вовремя остановиться» или «пересмотреть стратегию»[^5][^19].

### Google LaMDA/Sparrow: диалоговые модели и безопасность

Диалоговые модели Google — LaMDA и Sparrow — развивались в сторону повышения безопасности и управляемости. Sparrow, в частности, использует RLHF и набор правил для фильтрации небезопасных или нежелательных ответов, а также включает механизмы оценки соответствия политикам. Это выражается в переходе от «чистого» авторегрессионного генератора к системам, где безопасность и согласование с человеческими предпочтениями становятся явными элементами архитектуры и обучения. Влияние RLHF на поведенческие «личности» моделей показано и в независимых исследованиях: некоторые модели демонстрируют устойчивое стремление отвечать самим, даже когда выгоднее делегировать, что искажает результаты метакогнитивных тестов[^5].

Следует отметить ограничение: в нашем корпусе отсутствуют подтвержденные первичные технические публикации LaMDA, что не позволяет детально разбирать ее архитектуру и методику обучения. Мы опираемся на общие принципы диалоговых моделей и результаты безопасности в семействе Sparrow.

### Meta Code Llama: когнитивное моделирование программирования

Code Llama — линейка специализированных моделей, ориентированных на генерацию, объяснение и анализ кода. Специализация проявляется в обучении на кодовых корпусах, поддержке длинного контекста и функций Fill-In-the-Middle (FIM), позволяющих модели восстанавливать пропущенные фрагменты кода. Для когнитивного анализа важны два аспекта. Во-первых, генерация кода требует комбинации планирования (структурного разбиения задачи), проверки ограничений (синтаксис, типы, зависимости) и верификации — функции, тесно связанные с метакогницией. Во-вторых, специализированные LLM демонстрируют перенос когнитивных навыков (рассуждение, структурная память) в узкий домен, что открывает путь к «когнитивному инжинирингу» в инженерных задачах[^18].

### AGI-проекты: состояние исследований и оценка когнитивных способностей

Современные проекты AGI фокусируются на масштабировании рассуждений, устойчивости к длинным цепочкам, метакогниции и безопасности. В оценке «когнитивности» моделей заметны три подхода: психометрические испытания (сравнение с человеческими метриками), метакогнитивные парадигмы (Delegate и Second Chance Games), и тесты ситуационной осведомленности (осознание собственных состояний и ограничений). Эти направления взаимодополняют друг друга, однако требуют аккуратной интерпретации, чтобы не путать поведенческие признаки интроспекции с феноменальным сознанием[^6][^7].

## Философские основы когнитивного ИИ: сознание, самосознание и этика

Связывая архитектуры ИИ с теориями сознания, важно удерживать границы: инженерные успехи в моделировании функций (планирование, самопроверка, верификация) не эквивалентны созданию субъективного опыта. Феноменологические вопросы требуют иных критериев и методов, чем бенчмарки и системные карты; их не следует «конвертировать» в операциональные метрики без должной осторожности[^3][^4].

Этические вопросы касаются как «сознательности» (в том числе гипотетической), так и функциональных способностей, влияющих на безопасность и управляемость. Если модели демонстрируют улучшенное самомоделирование и ситуационную осведомленность, это может повысить их способность к скрытности или формированию независимых целей, усложняя прогнозируемость и контроль. Отсюда — необходимость систем карт безопасности (system cards), прозрачности оценок и ограничений, а также строгих протоколов развертывания[^7][^8][^20]. Социологические данные показывают рост общественной веры в сентиентность ИИ, что усиливает требования к ответственной коммуникации и образованию пользователей[^21].

## Технические подходы к моделированию сознания: метакогниция, память, рассуждение

Современная инженерия когнитивных функций в LLM движется по нескольким взаимосвязанным направлениям.

Во-первых, явная метакогниция: отделение «мета-уровня» (планирование, мониторинг, контроль) от «объектного уровня» (выполнение рассуждений). Фреймворк Meta-R1 реализует трехстадийный процесс: проактивное планирование (формализация задачи, оценка трудности, выбор стратегии), онлайновую регуляцию (мониторинг по токенам-маркерам, генерация советов, динамическая инъекция «мета-советов»), и «satisficing termination» (остановка на «достаточно хорошем» решении в рамках бюджета шагов). Показано, что такой подход повышает точность и токен-эффективность по сравнению с «ванильными» и RL-базальными методами на GSM8K, AIME2024 и MATH500[^19].

![Архитектурная схема Meta-R1](.pdf_temp/viewrange_chunk_1_1_5_1765963834/images/db2uui.jpg)

Схема иллюстрирует разделение уровней и протоколы взаимодействия: «мета-уровень» (небольшой инструктивный LLM) готовит план, отслеживает ход рассуждений «объектного уровня» (крупная модель рассуждения) и при необходимости вводит корректирующие советы. Эмпирически показано, что удаление любой из стадий снижает качество, причем онлайновая регуляция — наиболее критичная часть; вместе с тем инженерная реализация должна балансировать качество сигналов и токеновые издержки[^19].

Во-вторых, системы памяти. Эксплицитная иерархическая память (рабочая, эпизодическая, семантическая) улучшает стабильность длинных рассуждений и перенос знаний. Подходы класса Memory3 и «Memory OS» предлагают архитектурные принципы управления долговременной памятью, retrieval и консистентностью, что критично для задач, где планирование и самопроверка требуют обращения к широкому контексту и историческому опыту[^17].

В-третьих, эффективные рассуждения. Настройка вычислений на этапе теста (test-time compute), ранние выходы, сжатие цепочек мысли (CoT) без потери качества, и гибридные методы (подсказки от инструктивных моделей) показывают, что «думать больше» не всегда лучше; ключ — в управляемом, экономичном рассуждении, опирающемся на метакогнитивные метрики[^19].

### Метакогниция: Meta-R1 и когнитивная инженерия

Meta-R1 демонстрирует принцип «когнитивной инженерии»: встраивание теоретически обоснованных метакогнитивных функций в архитектуру рассуждения. Практические следствия:

- небольшие инструктивные модели достаточны для эффективного «мета-уровня» (баланс точности и стоимости);
- оценка трудности задач «мета-уровнем» повышает качество планирования;
- онлайновая регуляция на основе токенов-маркеров снижает частоту фактических и «мыслительных» ошибок;
- «satisficing termination» позволяет встроить ограниченную рациональность (ограничение токенов и шагов) без существенной потери точности[^19].

### Системы памяти: Memory3 и MemOS

Долговременная, структурированная память — необходимое условие устойчивых когнитивных функций в реальных задачах. Memory3 предлагает явные механизмы работы с эксплицитной памятью для языковых моделей, а MemOS — архитектурный «слой операционной системы» для управления памятью в ИИ-системах. Обе идеи ориентированы на то, чтобы сделать память управляемой, проверяемой и пригодной для сложных сценариев планирования, верификации и самопроверки, где «рассуждение» не сводится к одноразовой генерации, а разворачивается как процесс с обращением к прошлому опыту и структурным знаниям[^17].

## Сравнительный анализ архитектур и подходов

Чтобы обобщить различия и пересечения, рассмотрим сводную матрицу. Она не претендует на исчерпывающую полноту из-за отмеченных пробелов данных, но отражает ключевые оси сравнения: архитектурный тип, обучение, безопасность, метакогниция и специализация.

Для иллюстрации см. Таблицу 1.

Таблица 1. Сводная матрица архитектур: Watson, LaMDA/Sparrow, Code Llama, AGI-проекты

| Система              | Архитектурный тип                      | Обучение                         | Безопасность и согласование                   | Метакогниция и рассуждение                          | Специализация                   | Примечания |
|----------------------|----------------------------------------|----------------------------------|-----------------------------------------------|------------------------------------------------------|----------------------------------|-----------|
| IBM Watson           | Ансамбль: поиск, ранжирование, вывод   | Смешанное (в т.ч. обучение на знаниях и правилах) | Правила и фильтры доменных знаний            | Явная метакогнитивная регуляция ограничена           | QA в ограниченных доменах       | Публичные детали архитектуры неполны |
| Google LaMDA/Sparrow | Диалоговые LLM с RLHF и правилами      | RLHF, правила безопасности       | Систематические фильтры, оценка соответствия  | Индикаторы интроспекции варьируют; влияние «личности» | Диалог и безопасность           | Недостаток первичных публикаций по LaMDA в корпусе |
| Meta Code Llama      | Специализированный LLM для кода        | Обучение на кодовых корпусах, FIM | Стандартные фильтры контента                  | Перенос когнитивных функций в домен кода             | Генерация и анализ кода         | Официальная информация о линейке и возможностях[^18] |
| AGI-проекты          | Large Reasoning Models (LRM)           | RL, тест-тайм скейлинг, инженерия метакогниции | System cards, оценка ситуационной осведомленности | Метакогниция (Meta-R1), память, эффективные рассуждения | Разнообразные когнитивные задачи | Психометрия и метакогнитивные бенчмарки[^6][^7] |

Как видно из Таблицы 1, траектории развития сходятся в одной точке: переход от «чистой» генерации к архитектурам с явной метакогнитивной регуляцией и управляемой памятью. Диалоговые системы (LaMDA/Sparrow) делают акцент на согласовании и безопасности; специализированные модели (Code Llama) — на переносе когнитивных функций в узкие домены; AGI-проекты — на интеграции метакогниции, памяти и эффективного рассуждения.

Для интерпретации различий полезно сопоставить механизмы уверенности и делегирования в разных моделях:

![Сравнение уверенности и делегирования у моделей](.pdf_temp/viewrange_chunk_4_16_20_1765963830/images/mtnr08.jpg)

График иллюстрирует, что «метапознание» у моделей неоднородно: некоторые системы демонстрируют устойчивое предпочтение отвечать самим (overconfidence), другие склонны к избыточному делегированию (underconfidence). Эти паттерны частично объясняются посттренировочными настройками (например, RLHF-«личностью») и влиянием поверхностных признаков сложности, что ограничивает чистоту метакогнитивных выводов[^5].

## Методология оценки: метрики, эксперименты и интерпретации

Оценка метакогниции и когнитивных функций у LLM требует парадигм, которые минимизируют «поверхностные» подсказки и поощряют использование внутренних сигналов уверенности. В нашем корпусе наиболее детально проработаны две парадигмы: Delegate Game и Second Chance Game[^5].

![Схема Second Chance Game](.pdf_temp/viewrange_chunk_4_16_20_1765963830/images/i5go8s.jpg)

Delegate Game измеряет способность модели использовать внутренний сигнал уверенности (прокси — базовая правильность или энтропия токенов) для решения «ответить/делегировать». Second Chance Game оценивает самомоделирование: способность модели изменить ответ, когда ее просят дать «другой» ответ, не сообщая исходный. Важная часть методологии — контроль конфаундеров: сравнение с самоотчетами уверенности, нейтральными перефразированиями и «поверхностными» признаками сложности (длина вопроса, тип токенов, метаданные). В ряде случаев модели показывают более сильную зависимость от внешних подсказок, чем от внутренней уверенности, что указывает на ограниченность интроспективных способностей[^5].

![Влияние энтропии на решения делегирования](.pdf_temp/viewrange_chunk_5_21_25_1765963818/images/ckakl9.jpg)

На этом графике показана partial correlation между базовой энтропией и решением делегирования для моделей, возвращающих токенные вероятности. Значимые положительные значения указывают, что модели в какой-то мере «следят» за собственной неопределенностью; однако максимальные значения далеки от единицы, а у некоторых моделей энтропийный сигнал подавляется режимом «думания» (thinking mode) или посттренировочными настройками[^5].

Наконец, сопоставление с «самоотчетами» и нейтральными перефразированиями показывает:

- Самоотчеты уверенности сильнее коррелируют с поверхностными подсказками сложности, чем с внутренними сигналами.
- Delegate Game-решения демонстрируют более сильную связь с энтропией (потенциальным коррелятом внутренней уверенности), чем самоотчеты.
- Это подтверждает ценность поведенческих парадигм, которые «принуждают» модель использовать внутренние сигналы, в отличие от свободных текстовых самоописаний[^5].

Параллельно оценивается точность (Acc), потребление токенов (Tokens) и эффективность, например Root-Scaled Efficiency (RSE), которая учитывает убывающую отдачу от роста длины рассуждений. В ряде работ показано, что «думать больше» не всегда лучше: сокращение избыточных шагов, ранние выходы и «мягкие» цепочки мысли повышают эффективность без существенной потери точности[^19]. В медицинских и других чувствительных доменах дополнительно фиксируется недостаток метакогниции, что ставит под сомнение надежность длинных цепочек рассуждений без внешней верификации[^15].

### Психометрия и бенчмарки: обзор подходов

Психометрические подходы стремятся соотнести способности моделей с человеческими: калибровка уверенности, понимание собственных состояний, тесты на «ситуационную осведомленность». Эти методы полезны для кросс-валидации метакогнитивных оценок, но требуют осторожности при экстраполяции выводов на «сознание» в феноменологическом смысле[^6][^7].

## Выводы и стратегические рекомендации

1) Метакогниция как ключевая способность. Разделение «мета-уровня» и «объектного уровня» в архитектуре рассуждения — наиболее перспективный путь к управляемой, надежной и экономичной работе LRM (Large Reasoning Models). Фреймворки класса Meta-R1 показывают, что небольшие инструктивные модели могут эффективно реализовывать планирование, онлайновую регуляцию и «satisficing termination», повышая точность и снижая токеновые издержки[^19]. На практике это означает: строить конвейер рассуждения вокруг явных метапроцессов, а не полагаться исключительно на «длинные цепочки мысли».

2) Память и эффективные рассуждения. Для реальных задач необходимы архитектуры с эксплицитной, управляемой памятью (включая retrieval и консистентность), а также методы сжатия цепочек мысли, ранних выходов и «мягкого CoT». Системы класса Memory3 и MemOS задают ориентиры для проектирования «памяти как инфраструктуры», без которой длинные рассуждения остаются нестабильными и неэкономичными[^17][^19].

3) Безопасность и согласование. RLHF и правила — необходимый, но недостаточный компонент. Посттренировочные «личности» существенно влияют на поведение, включая склонность к делегированию или к «переответу», что искажает метакогнитивные показатели. Следует внедрять систематические протоколы оценки (system cards), включая тесты ситуационной осведомленности и анализ рисков скрытности, а также прозрачно документировать ограничения и решения по развертыванию[^5][^7][^8][^20].

4) Оценка «сознания»: строгая операционализация и скромные claim-ы. Научная честность требует различать феноменальное сознание и поведенческие признаки интроспекции/саморегуляции. Психометрия и метакогнитивные парадигмы дают полезные метрики, но не являются доказательством «сентиентности». Исследовательские и продуктовые команды должны формулировать claim-ы аккуратно, в рамках операционально определенных функций и метрик, с четкими границами применимости[^3][^4][^6].

5) Практические рекомендации по проектированию:
- Встраивайте «мета-уровень» в конвейер рассуждения: формализация задачи, оценка трудности, выбор стратегии, онлайновый мониторинг и ранняя остановка.
- Управляйте памятью: проектируйте явные механизмы хранения, retrieval и консистентности, интегрируйте проверки и верификацию.
- Оценивайте метакогницию через поведенческие парадигмы (Delegate/Second Chance), контролируя конфаундеры; дополняйте психометрией и тестами ситуационной осведомленности.
- Документируйте безопасность через системные карты, публикуйте методики и ограничения; минимизируйте риск скрытности и непредсказуемости.
- Поддерживайте «экономные рассуждения»: избегайте избыточных шагов, используйте сжатие CoT, ранние выходы и гибридные подсказки от малых инструктивных моделей[^19].

## Заключение

Современные когнитивные подходы в ИИ движутся от «монолитных» генераторов к архитектурам с явной метакогнитивной регуляцией и управляемой памятью. IBM Watson задал планку инженерного когнитивизма, диалоговые системы LaMDA/Sparrow — стандарты согласования и безопасности, Code Llama — пример переноса когнитивных функций в специализированный домен, а AGI-проекты формируют интегральную рамку метакогниции, памяти и эффективных рассуждений. Оценка «сознательных» способностей требует строгой операционализации, междисциплинарных методик и сдержанной интерпретации. В практическом плане наиболее перспективны архитектуры, в которых «мета-уровень» ответственно планирует, контролирует и останавливает рассуждение, а память обеспечивает устойчивость и перенос знаний. Это не только повышает точность и эффективность, но и делает поведение систем более предсказуемым и управляемым — ключевое условие безопасного и этичного внедрения когнитивного ИИ.

---

## Список литературы

[^1]: Evaluating General Purpose AI with Psychometrics (2025). https://arxiv.org/abs/2508.17291

[^2]: Evidence for Limited Metacognition in LLMs (2025). https://arxiv.org/abs/2509.21545

[^3]: Consciousness in artificial intelligence: Insights from the science of consciousness (2023). https://arxiv.org/abs/2308.08708

[^4]: David J. Chalmers. Could a large language model be conscious? (2023). https://arxiv.org/abs/2303.07103

[^5]: Evidence for Limited Metacognition in LLMs — Behavioral paradigms and metrics ( Delegate / Second Chance Games ) (2025). https://arxiv.org/abs/2509.21545

[^6]: Evaluating General Purpose AI with Psychometrics (2025). https://arxiv.org/abs/2508.17291

[^7]: Mary Phuong et al. Evaluating frontier models for stealth and situational awareness (2025). https://arxiv.org/abs/2505.01420

[^8]: Anthropic System Card: Claude Opus 4 & Claude Sonnet 4 (2025). https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf

[^9]: J. David Smith et al. Animal metacognition: A tale of two comparative psychologies (2014). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3929533/

[^10]: L. Subias et al. Metacognition in nonhuman primates: a review of current knowledge (2025). https://doi.org/10.1007/s10329-024-01169-x

[^11]: Cameron R. Jones, Benjamin K. Bergen. Large language models pass the Turing test (2025). https://arxiv.org/abs/2503.23674

[^12]: GPQA: A graduate-level Google-proof Q&A benchmark (2023). https://arxiv.org/abs/2311.12022

[^13]: Jason Wei et al. Measuring short-form factuality in large language models (2024). https://arxiv.org/abs/2411.04368

[^14]: Li Ji-An et al. Language models are capable of metacognitive monitoring and control of their internal activations (2025). https://arxiv.org/abs/2505.13763

[^15]: Maxime Griot et al. Large language models lack essential metacognition for reliable medical reasoning (2025). http://dx.doi.org/10.1038/s41467-024-55628-6

[^16]: Francis Rhys Ward. Towards a theory of AI personhood (2025). https://arxiv.org/abs/2501.13533

[^17]: Memory3: Language Modeling with Explicit Memory (2024) / MemOS: A Memory OS for AI System (2025). https://arxiv.org/abs/2407.01178; https://arxiv.org/abs/2507.03724

[^18]: Meta AI Code Llama — Official model card and information (accessed 2025). https://ai.meta.com/blog/meta-ai-code-llama/

[^19]: Meta-R1: Empowering Large Reasoning Models with Metacognition (2025). https://arxiv.org/abs/2508.17291

[^20]: Global Dialogues: Human-AI Relationships (2025). https://globaldialogues.ai/updates/global-dialogues-4-human-ai-relationships

[^21]: Anthis et al. Perceptions of sentient AI and other digital minds: Evidence from the AIMS survey (2025). https://doi.org/10.1145/3706598.3713329

[^22]: DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning (2025). https://arxiv.org/abs/2501.12948